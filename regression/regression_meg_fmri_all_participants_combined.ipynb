{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import os\n",
    "from functools import partial \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '/scratch/jd5697/cv_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6240\n",
      "2400\n",
      "100\n",
      "8740\n"
     ]
    }
   ],
   "source": [
    "embeddings_train = np.load(os.path.join(dir_name, 'image_embeddings_vit_train_fmri_subset.npy'), allow_pickle=True).item()\n",
    "embeddings_test = np.load(os.path.join(dir_name, 'image_embeddings_vit_test_fmri_subset.npy'), allow_pickle=True).item()\n",
    "embeddings_test_small = np.load(os.path.join(dir_name, 'image_embeddings_vit_test_small_fmri_subset.npy'), allow_pickle=True).item()\n",
    "\n",
    "print(len(embeddings_train))\n",
    "print(len(embeddings_test))\n",
    "print(len(embeddings_test_small))\n",
    "print(len(embeddings_train) + len(embeddings_test) + len(embeddings_test_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74880\n",
      "28800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "#check shape\n",
    "with open('valid_epochs_all_train_meg_fmri_combined.pickle', 'rb') as f:\n",
    "    epochs_train = pickle.load(f)\n",
    "\n",
    "with open('valid_epochs_all_test_meg_fmri_combined.pickle', 'rb') as f:\n",
    "    epochs_test = pickle.load(f)\n",
    "\n",
    "with open('valid_epochs_all_test_small_meg_fmri_combined.pickle', 'rb') as f:\n",
    "    epochs_test_small = pickle.load(f)\n",
    "\n",
    "print(len(epochs_train))\n",
    "print(len(epochs_test))\n",
    "print(len(epochs_test_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74880, 57972)\n",
      "(28800, 57972)\n",
      "(1200, 57972)\n"
     ]
    }
   ],
   "source": [
    "data_train = epochs_train.get_data()\n",
    "data_train = data_train.reshape(data_train.shape[0], -1)\n",
    "data_train = np.hstack((data_train, np.vstack(epochs_train.metadata['svd'])))\n",
    "\n",
    "data_test = epochs_test.get_data()\n",
    "data_test = data_test.reshape(data_test.shape[0], -1)\n",
    "data_test = np.hstack((data_test, np.vstack(epochs_test.metadata['svd'])))\n",
    "\n",
    "data_test_small = epochs_test_small.get_data()\n",
    "data_test_small = data_test_small.reshape(data_test_small.shape[0], -1)\n",
    "data_test_small = np.hstack((data_test_small, np.vstack(epochs_test_small.metadata['svd'])))\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "print(data_test_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74880, 768)\n",
      "(28800, 768)\n",
      "(1200, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings_data_train = np.vstack([embeddings_train[key] for key in epochs_train.metadata['image_path']])\n",
    "embeddings_data_test = np.vstack([embeddings_test[key] for key in epochs_test.metadata['image_path']])\n",
    "embeddings_data_test_small = np.vstack([embeddings_test_small[key] for key in epochs_test_small.metadata['image_path']])\n",
    "\n",
    "print(embeddings_data_train.shape)\n",
    "print(embeddings_data_test.shape)\n",
    "print(embeddings_data_test_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del epochs_train, epochs_test, epochs_test_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "embeddings_data_train = scaler.fit_transform(embeddings_data_train)\n",
    "embeddings_data_test = scaler.transform(embeddings_data_test)\n",
    "embeddings_data_test_small = scaler.transform(embeddings_data_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = data_train.shape[1]\n",
    "output_size = embeddings_data_train.shape[1]\n",
    "max_epochs = 5000\n",
    "learning_rate = 0.003\n",
    "batch_size = 128\n",
    "patience = 2\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare data\n",
    "data_tensor = torch.tensor(data_train, dtype=torch.float32).to(device)\n",
    "target_tensor = torch.tensor(embeddings_data_train, dtype=torch.float32).to(device)\n",
    "dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MLPRegressor(input_size, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Learning rate schedule\n",
    "#scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 1 / (1 + 0.001 * epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model():\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{max_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Update learning rate\n",
    "        #scheduler.step()\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5000], Train Loss: 1.0689, Val Loss: 1.0267\n",
      "Epoch [2/5000], Train Loss: 0.9915, Val Loss: 0.9949\n",
      "Epoch [3/5000], Train Loss: 0.9540, Val Loss: 0.9741\n",
      "Epoch [4/5000], Train Loss: 0.9257, Val Loss: 0.9577\n",
      "Epoch [5/5000], Train Loss: 0.9020, Val Loss: 0.9431\n",
      "Epoch [6/5000], Train Loss: 0.8811, Val Loss: 0.9306\n",
      "Epoch [7/5000], Train Loss: 0.8622, Val Loss: 0.9193\n",
      "Epoch [8/5000], Train Loss: 0.8448, Val Loss: 0.9083\n",
      "Epoch [9/5000], Train Loss: 0.8288, Val Loss: 0.8980\n",
      "Epoch [10/5000], Train Loss: 0.8137, Val Loss: 0.8889\n",
      "Epoch [11/5000], Train Loss: 0.7996, Val Loss: 0.8801\n",
      "Epoch [12/5000], Train Loss: 0.7861, Val Loss: 0.8715\n",
      "Epoch [13/5000], Train Loss: 0.7733, Val Loss: 0.8635\n",
      "Epoch [14/5000], Train Loss: 0.7611, Val Loss: 0.8560\n",
      "Epoch [15/5000], Train Loss: 0.7494, Val Loss: 0.8479\n",
      "Epoch [16/5000], Train Loss: 0.7382, Val Loss: 0.8408\n",
      "Epoch [17/5000], Train Loss: 0.7274, Val Loss: 0.8332\n",
      "Epoch [18/5000], Train Loss: 0.7170, Val Loss: 0.8268\n",
      "Epoch [19/5000], Train Loss: 0.7071, Val Loss: 0.8199\n",
      "Epoch [20/5000], Train Loss: 0.6973, Val Loss: 0.8134\n",
      "Epoch [21/5000], Train Loss: 0.6880, Val Loss: 0.8074\n",
      "Epoch [22/5000], Train Loss: 0.6789, Val Loss: 0.8010\n",
      "Epoch [23/5000], Train Loss: 0.6701, Val Loss: 0.7951\n",
      "Epoch [24/5000], Train Loss: 0.6615, Val Loss: 0.7897\n",
      "Epoch [25/5000], Train Loss: 0.6532, Val Loss: 0.7841\n",
      "Epoch [26/5000], Train Loss: 0.6452, Val Loss: 0.7783\n",
      "Epoch [27/5000], Train Loss: 0.6373, Val Loss: 0.7728\n",
      "Epoch [28/5000], Train Loss: 0.6296, Val Loss: 0.7678\n",
      "Epoch [29/5000], Train Loss: 0.6222, Val Loss: 0.7621\n",
      "Epoch [30/5000], Train Loss: 0.6150, Val Loss: 0.7573\n",
      "Epoch [31/5000], Train Loss: 0.6079, Val Loss: 0.7521\n",
      "Epoch [32/5000], Train Loss: 0.6010, Val Loss: 0.7472\n",
      "Epoch [33/5000], Train Loss: 0.5942, Val Loss: 0.7425\n",
      "Epoch [34/5000], Train Loss: 0.5876, Val Loss: 0.7380\n",
      "Epoch [35/5000], Train Loss: 0.5812, Val Loss: 0.7334\n",
      "Epoch [36/5000], Train Loss: 0.5749, Val Loss: 0.7290\n",
      "Epoch [37/5000], Train Loss: 0.5687, Val Loss: 0.7248\n",
      "Epoch [38/5000], Train Loss: 0.5627, Val Loss: 0.7201\n",
      "Epoch [39/5000], Train Loss: 0.5568, Val Loss: 0.7161\n",
      "Epoch [40/5000], Train Loss: 0.5510, Val Loss: 0.7119\n",
      "Epoch [41/5000], Train Loss: 0.5454, Val Loss: 0.7076\n",
      "Epoch [42/5000], Train Loss: 0.5398, Val Loss: 0.7038\n",
      "Epoch [43/5000], Train Loss: 0.5344, Val Loss: 0.6995\n",
      "Epoch [44/5000], Train Loss: 0.5291, Val Loss: 0.6955\n",
      "Epoch [45/5000], Train Loss: 0.5239, Val Loss: 0.6918\n",
      "Epoch [46/5000], Train Loss: 0.5188, Val Loss: 0.6880\n",
      "Epoch [47/5000], Train Loss: 0.5138, Val Loss: 0.6844\n",
      "Epoch [48/5000], Train Loss: 0.5089, Val Loss: 0.6806\n",
      "Epoch [49/5000], Train Loss: 0.5040, Val Loss: 0.6770\n",
      "Epoch [50/5000], Train Loss: 0.4993, Val Loss: 0.6732\n",
      "Epoch [51/5000], Train Loss: 0.4946, Val Loss: 0.6698\n",
      "Epoch [52/5000], Train Loss: 0.4901, Val Loss: 0.6664\n",
      "Epoch [53/5000], Train Loss: 0.4856, Val Loss: 0.6629\n",
      "Epoch [54/5000], Train Loss: 0.4812, Val Loss: 0.6595\n",
      "Epoch [55/5000], Train Loss: 0.4769, Val Loss: 0.6566\n",
      "Epoch [56/5000], Train Loss: 0.4726, Val Loss: 0.6530\n",
      "Epoch [57/5000], Train Loss: 0.4684, Val Loss: 0.6499\n",
      "Epoch [58/5000], Train Loss: 0.4643, Val Loss: 0.6464\n",
      "Epoch [59/5000], Train Loss: 0.4603, Val Loss: 0.6436\n",
      "Epoch [60/5000], Train Loss: 0.4563, Val Loss: 0.6404\n",
      "Epoch [61/5000], Train Loss: 0.4524, Val Loss: 0.6373\n",
      "Epoch [62/5000], Train Loss: 0.4486, Val Loss: 0.6342\n",
      "Epoch [63/5000], Train Loss: 0.4448, Val Loss: 0.6314\n",
      "Epoch [64/5000], Train Loss: 0.4410, Val Loss: 0.6284\n",
      "Epoch [65/5000], Train Loss: 0.4374, Val Loss: 0.6255\n",
      "Epoch [66/5000], Train Loss: 0.4338, Val Loss: 0.6226\n",
      "Epoch [67/5000], Train Loss: 0.4303, Val Loss: 0.6199\n",
      "Epoch [68/5000], Train Loss: 0.4268, Val Loss: 0.6171\n",
      "Epoch [69/5000], Train Loss: 0.4233, Val Loss: 0.6142\n",
      "Epoch [70/5000], Train Loss: 0.4199, Val Loss: 0.6117\n",
      "Epoch [71/5000], Train Loss: 0.4167, Val Loss: 0.6091\n",
      "Epoch [72/5000], Train Loss: 0.4134, Val Loss: 0.6063\n",
      "Epoch [73/5000], Train Loss: 0.4101, Val Loss: 0.6038\n",
      "Epoch [74/5000], Train Loss: 0.4070, Val Loss: 0.6010\n",
      "Epoch [75/5000], Train Loss: 0.4038, Val Loss: 0.5985\n",
      "Epoch [76/5000], Train Loss: 0.4008, Val Loss: 0.5959\n",
      "Epoch [77/5000], Train Loss: 0.3977, Val Loss: 0.5934\n",
      "Epoch [78/5000], Train Loss: 0.3947, Val Loss: 0.5912\n",
      "Epoch [79/5000], Train Loss: 0.3918, Val Loss: 0.5886\n",
      "Epoch [80/5000], Train Loss: 0.3888, Val Loss: 0.5861\n",
      "Epoch [81/5000], Train Loss: 0.3860, Val Loss: 0.5838\n",
      "Epoch [82/5000], Train Loss: 0.3831, Val Loss: 0.5815\n",
      "Epoch [83/5000], Train Loss: 0.3803, Val Loss: 0.5791\n",
      "Epoch [84/5000], Train Loss: 0.3776, Val Loss: 0.5769\n",
      "Epoch [85/5000], Train Loss: 0.3749, Val Loss: 0.5747\n",
      "Epoch [86/5000], Train Loss: 0.3722, Val Loss: 0.5724\n",
      "Epoch [87/5000], Train Loss: 0.3696, Val Loss: 0.5702\n",
      "Epoch [88/5000], Train Loss: 0.3669, Val Loss: 0.5680\n",
      "Epoch [89/5000], Train Loss: 0.3644, Val Loss: 0.5658\n",
      "Epoch [90/5000], Train Loss: 0.3618, Val Loss: 0.5637\n",
      "Epoch [91/5000], Train Loss: 0.3593, Val Loss: 0.5615\n",
      "Epoch [92/5000], Train Loss: 0.3568, Val Loss: 0.5594\n",
      "Epoch [93/5000], Train Loss: 0.3544, Val Loss: 0.5575\n",
      "Epoch [94/5000], Train Loss: 0.3520, Val Loss: 0.5554\n",
      "Epoch [95/5000], Train Loss: 0.3496, Val Loss: 0.5534\n",
      "Epoch [96/5000], Train Loss: 0.3473, Val Loss: 0.5511\n",
      "Epoch [97/5000], Train Loss: 0.3450, Val Loss: 0.5493\n",
      "Epoch [98/5000], Train Loss: 0.3427, Val Loss: 0.5473\n",
      "Epoch [99/5000], Train Loss: 0.3405, Val Loss: 0.5454\n",
      "Epoch [100/5000], Train Loss: 0.3382, Val Loss: 0.5434\n",
      "Epoch [101/5000], Train Loss: 0.3360, Val Loss: 0.5414\n",
      "Epoch [102/5000], Train Loss: 0.3338, Val Loss: 0.5396\n",
      "Epoch [103/5000], Train Loss: 0.3317, Val Loss: 0.5377\n",
      "Epoch [104/5000], Train Loss: 0.3296, Val Loss: 0.5358\n",
      "Epoch [105/5000], Train Loss: 0.3275, Val Loss: 0.5340\n",
      "Epoch [106/5000], Train Loss: 0.3254, Val Loss: 0.5322\n",
      "Epoch [107/5000], Train Loss: 0.3233, Val Loss: 0.5304\n",
      "Epoch [108/5000], Train Loss: 0.3213, Val Loss: 0.5286\n",
      "Epoch [109/5000], Train Loss: 0.3193, Val Loss: 0.5269\n",
      "Epoch [110/5000], Train Loss: 0.3174, Val Loss: 0.5252\n",
      "Epoch [111/5000], Train Loss: 0.3154, Val Loss: 0.5234\n",
      "Epoch [112/5000], Train Loss: 0.3135, Val Loss: 0.5216\n",
      "Epoch [113/5000], Train Loss: 0.3116, Val Loss: 0.5200\n",
      "Epoch [114/5000], Train Loss: 0.3097, Val Loss: 0.5184\n",
      "Epoch [115/5000], Train Loss: 0.3079, Val Loss: 0.5167\n",
      "Epoch [116/5000], Train Loss: 0.3060, Val Loss: 0.5150\n",
      "Epoch [117/5000], Train Loss: 0.3042, Val Loss: 0.5133\n",
      "Epoch [118/5000], Train Loss: 0.3024, Val Loss: 0.5118\n",
      "Epoch [119/5000], Train Loss: 0.3006, Val Loss: 0.5101\n",
      "Epoch [120/5000], Train Loss: 0.2989, Val Loss: 0.5085\n",
      "Epoch [121/5000], Train Loss: 0.2971, Val Loss: 0.5069\n",
      "Epoch [122/5000], Train Loss: 0.2954, Val Loss: 0.5054\n",
      "Epoch [123/5000], Train Loss: 0.2937, Val Loss: 0.5038\n",
      "Epoch [124/5000], Train Loss: 0.2921, Val Loss: 0.5024\n",
      "Epoch [125/5000], Train Loss: 0.2904, Val Loss: 0.5009\n",
      "Epoch [126/5000], Train Loss: 0.2887, Val Loss: 0.4993\n",
      "Epoch [127/5000], Train Loss: 0.2871, Val Loss: 0.4979\n",
      "Epoch [128/5000], Train Loss: 0.2855, Val Loss: 0.4965\n",
      "Epoch [129/5000], Train Loss: 0.2839, Val Loss: 0.4949\n",
      "Epoch [130/5000], Train Loss: 0.2823, Val Loss: 0.4934\n",
      "Epoch [131/5000], Train Loss: 0.2808, Val Loss: 0.4918\n",
      "Epoch [132/5000], Train Loss: 0.2792, Val Loss: 0.4906\n",
      "Epoch [133/5000], Train Loss: 0.2777, Val Loss: 0.4890\n",
      "Epoch [134/5000], Train Loss: 0.2762, Val Loss: 0.4877\n",
      "Epoch [135/5000], Train Loss: 0.2747, Val Loss: 0.4864\n",
      "Epoch [136/5000], Train Loss: 0.2733, Val Loss: 0.4850\n",
      "Epoch [137/5000], Train Loss: 0.2718, Val Loss: 0.4836\n",
      "Epoch [138/5000], Train Loss: 0.2703, Val Loss: 0.4823\n",
      "Epoch [139/5000], Train Loss: 0.2689, Val Loss: 0.4809\n",
      "Epoch [140/5000], Train Loss: 0.2675, Val Loss: 0.4796\n",
      "Epoch [141/5000], Train Loss: 0.2661, Val Loss: 0.4782\n",
      "Epoch [142/5000], Train Loss: 0.2647, Val Loss: 0.4769\n",
      "Epoch [143/5000], Train Loss: 0.2633, Val Loss: 0.4756\n",
      "Epoch [144/5000], Train Loss: 0.2620, Val Loss: 0.4743\n",
      "Epoch [145/5000], Train Loss: 0.2606, Val Loss: 0.4731\n",
      "Epoch [146/5000], Train Loss: 0.2593, Val Loss: 0.4718\n",
      "Epoch [147/5000], Train Loss: 0.2580, Val Loss: 0.4705\n",
      "Epoch [148/5000], Train Loss: 0.2567, Val Loss: 0.4694\n",
      "Epoch [149/5000], Train Loss: 0.2554, Val Loss: 0.4681\n",
      "Epoch [150/5000], Train Loss: 0.2541, Val Loss: 0.4668\n",
      "Epoch [151/5000], Train Loss: 0.2528, Val Loss: 0.4655\n",
      "Epoch [152/5000], Train Loss: 0.2515, Val Loss: 0.4644\n",
      "Epoch [153/5000], Train Loss: 0.2503, Val Loss: 0.4632\n",
      "Epoch [154/5000], Train Loss: 0.2491, Val Loss: 0.4620\n",
      "Epoch [155/5000], Train Loss: 0.2479, Val Loss: 0.4607\n",
      "Epoch [156/5000], Train Loss: 0.2467, Val Loss: 0.4596\n",
      "Epoch [157/5000], Train Loss: 0.2455, Val Loss: 0.4585\n",
      "Epoch [158/5000], Train Loss: 0.2443, Val Loss: 0.4574\n",
      "Epoch [159/5000], Train Loss: 0.2431, Val Loss: 0.4563\n",
      "Epoch [160/5000], Train Loss: 0.2419, Val Loss: 0.4551\n",
      "Epoch [161/5000], Train Loss: 0.2408, Val Loss: 0.4540\n",
      "Epoch [162/5000], Train Loss: 0.2396, Val Loss: 0.4529\n",
      "Epoch [163/5000], Train Loss: 0.2385, Val Loss: 0.4516\n",
      "Epoch [164/5000], Train Loss: 0.2374, Val Loss: 0.4506\n",
      "Epoch [165/5000], Train Loss: 0.2363, Val Loss: 0.4494\n",
      "Epoch [166/5000], Train Loss: 0.2351, Val Loss: 0.4484\n",
      "Epoch [167/5000], Train Loss: 0.2341, Val Loss: 0.4472\n",
      "Epoch [168/5000], Train Loss: 0.2330, Val Loss: 0.4462\n",
      "Epoch [169/5000], Train Loss: 0.2319, Val Loss: 0.4451\n",
      "Epoch [170/5000], Train Loss: 0.2308, Val Loss: 0.4441\n",
      "Epoch [171/5000], Train Loss: 0.2298, Val Loss: 0.4430\n",
      "Epoch [172/5000], Train Loss: 0.2287, Val Loss: 0.4422\n",
      "Epoch [173/5000], Train Loss: 0.2277, Val Loss: 0.4411\n",
      "Epoch [174/5000], Train Loss: 0.2267, Val Loss: 0.4400\n",
      "Epoch [175/5000], Train Loss: 0.2257, Val Loss: 0.4389\n",
      "Epoch [176/5000], Train Loss: 0.2246, Val Loss: 0.4379\n",
      "Epoch [177/5000], Train Loss: 0.2236, Val Loss: 0.4369\n",
      "Epoch [178/5000], Train Loss: 0.2227, Val Loss: 0.4359\n",
      "Epoch [179/5000], Train Loss: 0.2217, Val Loss: 0.4350\n",
      "Epoch [180/5000], Train Loss: 0.2207, Val Loss: 0.4340\n",
      "Epoch [181/5000], Train Loss: 0.2197, Val Loss: 0.4330\n",
      "Epoch [182/5000], Train Loss: 0.2188, Val Loss: 0.4321\n",
      "Epoch [183/5000], Train Loss: 0.2178, Val Loss: 0.4311\n",
      "Epoch [184/5000], Train Loss: 0.2169, Val Loss: 0.4300\n",
      "Epoch [185/5000], Train Loss: 0.2159, Val Loss: 0.4292\n",
      "Epoch [186/5000], Train Loss: 0.2150, Val Loss: 0.4282\n",
      "Epoch [187/5000], Train Loss: 0.2141, Val Loss: 0.4272\n",
      "Epoch [188/5000], Train Loss: 0.2132, Val Loss: 0.4264\n",
      "Epoch [189/5000], Train Loss: 0.2123, Val Loss: 0.4254\n",
      "Epoch [190/5000], Train Loss: 0.2114, Val Loss: 0.4245\n",
      "Epoch [191/5000], Train Loss: 0.2105, Val Loss: 0.4235\n",
      "Epoch [192/5000], Train Loss: 0.2096, Val Loss: 0.4227\n",
      "Epoch [193/5000], Train Loss: 0.2087, Val Loss: 0.4217\n",
      "Epoch [194/5000], Train Loss: 0.2079, Val Loss: 0.4209\n",
      "Epoch [195/5000], Train Loss: 0.2070, Val Loss: 0.4200\n",
      "Epoch [196/5000], Train Loss: 0.2062, Val Loss: 0.4191\n",
      "Epoch [197/5000], Train Loss: 0.2053, Val Loss: 0.4182\n",
      "Epoch [198/5000], Train Loss: 0.2045, Val Loss: 0.4174\n",
      "Epoch [199/5000], Train Loss: 0.2036, Val Loss: 0.4165\n",
      "Epoch [200/5000], Train Loss: 0.2028, Val Loss: 0.4156\n",
      "Epoch [201/5000], Train Loss: 0.2020, Val Loss: 0.4148\n",
      "Epoch [202/5000], Train Loss: 0.2012, Val Loss: 0.4140\n",
      "Epoch [203/5000], Train Loss: 0.2004, Val Loss: 0.4131\n",
      "Epoch [204/5000], Train Loss: 0.1996, Val Loss: 0.4123\n",
      "Epoch [205/5000], Train Loss: 0.1988, Val Loss: 0.4114\n",
      "Epoch [206/5000], Train Loss: 0.1980, Val Loss: 0.4107\n",
      "Epoch [207/5000], Train Loss: 0.1972, Val Loss: 0.4099\n",
      "Epoch [208/5000], Train Loss: 0.1964, Val Loss: 0.4090\n",
      "Epoch [209/5000], Train Loss: 0.1957, Val Loss: 0.4081\n",
      "Epoch [210/5000], Train Loss: 0.1949, Val Loss: 0.4074\n",
      "Epoch [211/5000], Train Loss: 0.1942, Val Loss: 0.4065\n",
      "Epoch [212/5000], Train Loss: 0.1934, Val Loss: 0.4058\n",
      "Epoch [213/5000], Train Loss: 0.1926, Val Loss: 0.4049\n",
      "Epoch [214/5000], Train Loss: 0.1919, Val Loss: 0.4041\n",
      "Epoch [215/5000], Train Loss: 0.1912, Val Loss: 0.4033\n",
      "Epoch [216/5000], Train Loss: 0.1904, Val Loss: 0.4026\n",
      "Epoch [217/5000], Train Loss: 0.1897, Val Loss: 0.4018\n",
      "Epoch [218/5000], Train Loss: 0.1890, Val Loss: 0.4011\n",
      "Epoch [219/5000], Train Loss: 0.1883, Val Loss: 0.4004\n",
      "Epoch [220/5000], Train Loss: 0.1876, Val Loss: 0.3996\n",
      "Epoch [221/5000], Train Loss: 0.1869, Val Loss: 0.3988\n",
      "Epoch [222/5000], Train Loss: 0.1862, Val Loss: 0.3981\n",
      "Epoch [223/5000], Train Loss: 0.1855, Val Loss: 0.3973\n",
      "Epoch [224/5000], Train Loss: 0.1848, Val Loss: 0.3966\n",
      "Epoch [225/5000], Train Loss: 0.1841, Val Loss: 0.3958\n",
      "Epoch [226/5000], Train Loss: 0.1834, Val Loss: 0.3951\n",
      "Epoch [227/5000], Train Loss: 0.1828, Val Loss: 0.3944\n",
      "Epoch [228/5000], Train Loss: 0.1821, Val Loss: 0.3937\n",
      "Epoch [229/5000], Train Loss: 0.1814, Val Loss: 0.3930\n",
      "Epoch [230/5000], Train Loss: 0.1808, Val Loss: 0.3922\n",
      "Epoch [231/5000], Train Loss: 0.1801, Val Loss: 0.3915\n",
      "Epoch [232/5000], Train Loss: 0.1795, Val Loss: 0.3908\n",
      "Epoch [233/5000], Train Loss: 0.1788, Val Loss: 0.3901\n",
      "Epoch [234/5000], Train Loss: 0.1782, Val Loss: 0.3894\n",
      "Epoch [235/5000], Train Loss: 0.1775, Val Loss: 0.3887\n",
      "Epoch [236/5000], Train Loss: 0.1769, Val Loss: 0.3879\n",
      "Epoch [237/5000], Train Loss: 0.1763, Val Loss: 0.3873\n",
      "Epoch [238/5000], Train Loss: 0.1757, Val Loss: 0.3866\n",
      "Epoch [239/5000], Train Loss: 0.1750, Val Loss: 0.3859\n",
      "Epoch [240/5000], Train Loss: 0.1744, Val Loss: 0.3852\n",
      "Epoch [241/5000], Train Loss: 0.1738, Val Loss: 0.3847\n",
      "Epoch [242/5000], Train Loss: 0.1732, Val Loss: 0.3839\n",
      "Epoch [243/5000], Train Loss: 0.1726, Val Loss: 0.3832\n",
      "Epoch [244/5000], Train Loss: 0.1720, Val Loss: 0.3826\n",
      "Epoch [245/5000], Train Loss: 0.1714, Val Loss: 0.3820\n",
      "Epoch [246/5000], Train Loss: 0.1708, Val Loss: 0.3813\n",
      "Epoch [247/5000], Train Loss: 0.1702, Val Loss: 0.3806\n",
      "Epoch [248/5000], Train Loss: 0.1696, Val Loss: 0.3801\n",
      "Epoch [249/5000], Train Loss: 0.1691, Val Loss: 0.3793\n",
      "Epoch [250/5000], Train Loss: 0.1685, Val Loss: 0.3786\n",
      "Epoch [251/5000], Train Loss: 0.1679, Val Loss: 0.3780\n",
      "Epoch [252/5000], Train Loss: 0.1674, Val Loss: 0.3774\n",
      "Epoch [253/5000], Train Loss: 0.1668, Val Loss: 0.3767\n",
      "Epoch [254/5000], Train Loss: 0.1662, Val Loss: 0.3761\n",
      "Epoch [255/5000], Train Loss: 0.1657, Val Loss: 0.3756\n",
      "Epoch [256/5000], Train Loss: 0.1651, Val Loss: 0.3749\n",
      "Epoch [257/5000], Train Loss: 0.1646, Val Loss: 0.3743\n",
      "Epoch [258/5000], Train Loss: 0.1640, Val Loss: 0.3736\n",
      "Epoch [259/5000], Train Loss: 0.1635, Val Loss: 0.3730\n",
      "Epoch [260/5000], Train Loss: 0.1629, Val Loss: 0.3725\n",
      "Epoch [261/5000], Train Loss: 0.1624, Val Loss: 0.3719\n",
      "Epoch [262/5000], Train Loss: 0.1619, Val Loss: 0.3713\n",
      "Epoch [263/5000], Train Loss: 0.1614, Val Loss: 0.3707\n",
      "Epoch [264/5000], Train Loss: 0.1608, Val Loss: 0.3700\n",
      "Epoch [239/10000], Train Loss: 0.1125, Val Loss: 0.3088\n",
      "Epoch [240/10000], Train Loss: 0.1121, Val Loss: 0.3082\n",
      "Epoch [241/10000], Train Loss: 0.1117, Val Loss: 0.3076\n",
      "Epoch [242/10000], Train Loss: 0.1112, Val Loss: 0.3072\n",
      "Epoch [243/10000], Train Loss: 0.1108, Val Loss: 0.3066\n",
      "Epoch [244/10000], Train Loss: 0.1104, Val Loss: 0.3061\n",
      "Epoch [245/10000], Train Loss: 0.1100, Val Loss: 0.3054\n",
      "Epoch [246/10000], Train Loss: 0.1095, Val Loss: 0.3049\n",
      "Epoch [247/10000], Train Loss: 0.1091, Val Loss: 0.3043\n",
      "Epoch [248/10000], Train Loss: 0.1087, Val Loss: 0.3037\n",
      "Epoch [249/10000], Train Loss: 0.1083, Val Loss: 0.3032\n",
      "Epoch [250/10000], Train Loss: 0.1079, Val Loss: 0.3027\n",
      "Epoch [251/10000], Train Loss: 0.1075, Val Loss: 0.3022\n",
      "Epoch [252/10000], Train Loss: 0.1071, Val Loss: 0.3016\n",
      "Epoch [253/10000], Train Loss: 0.1067, Val Loss: 0.3012\n",
      "Epoch [254/10000], Train Loss: 0.1063, Val Loss: 0.3006\n",
      "Epoch [255/10000], Train Loss: 0.1059, Val Loss: 0.3002\n",
      "Epoch [256/10000], Train Loss: 0.1056, Val Loss: 0.2995\n",
      "Epoch [257/10000], Train Loss: 0.1052, Val Loss: 0.2991\n",
      "Epoch [258/10000], Train Loss: 0.1048, Val Loss: 0.2985\n",
      "Epoch [259/10000], Train Loss: 0.1044, Val Loss: 0.2981\n",
      "Epoch [260/10000], Train Loss: 0.1040, Val Loss: 0.2976\n",
      "Epoch [261/10000], Train Loss: 0.1037, Val Loss: 0.2970\n",
      "Epoch [262/10000], Train Loss: 0.1033, Val Loss: 0.2965\n",
      "Epoch [263/10000], Train Loss: 0.1029, Val Loss: 0.2960\n",
      "Epoch [264/10000], Train Loss: 0.1026, Val Loss: 0.2956\n",
      "Epoch [265/10000], Train Loss: 0.1022, Val Loss: 0.2950\n",
      "Epoch [266/10000], Train Loss: 0.1018, Val Loss: 0.2945\n",
      "Epoch [267/10000], Train Loss: 0.1015, Val Loss: 0.2940\n",
      "Epoch [268/10000], Train Loss: 0.1011, Val Loss: 0.2936\n",
      "Epoch [269/10000], Train Loss: 0.1007, Val Loss: 0.2930\n",
      "Epoch [270/10000], Train Loss: 0.1004, Val Loss: 0.2926\n",
      "Epoch [271/10000], Train Loss: 0.1000, Val Loss: 0.2922\n",
      "Epoch [272/10000], Train Loss: 0.0997, Val Loss: 0.2917\n",
      "Epoch [273/10000], Train Loss: 0.0994, Val Loss: 0.2912\n",
      "Epoch [274/10000], Train Loss: 0.0990, Val Loss: 0.2907\n",
      "Epoch [275/10000], Train Loss: 0.0987, Val Loss: 0.2903\n",
      "Epoch [276/10000], Train Loss: 0.0983, Val Loss: 0.2897\n",
      "Epoch [277/10000], Train Loss: 0.0980, Val Loss: 0.2893\n",
      "Epoch [278/10000], Train Loss: 0.0977, Val Loss: 0.2888\n",
      "Epoch [279/10000], Train Loss: 0.0973, Val Loss: 0.2883\n",
      "Epoch [280/10000], Train Loss: 0.0970, Val Loss: 0.2879\n",
      "Epoch [281/10000], Train Loss: 0.0967, Val Loss: 0.2873\n",
      "Epoch [282/10000], Train Loss: 0.0963, Val Loss: 0.2870\n",
      "Epoch [283/10000], Train Loss: 0.0960, Val Loss: 0.2865\n",
      "Epoch [284/10000], Train Loss: 0.0957, Val Loss: 0.2860\n",
      "Epoch [285/10000], Train Loss: 0.0954, Val Loss: 0.2856\n",
      "Epoch [286/10000], Train Loss: 0.0950, Val Loss: 0.2851\n",
      "Epoch [287/10000], Train Loss: 0.0947, Val Loss: 0.2848\n",
      "Epoch [288/10000], Train Loss: 0.0944, Val Loss: 0.2843\n",
      "Epoch [289/10000], Train Loss: 0.0941, Val Loss: 0.2838\n",
      "Epoch [290/10000], Train Loss: 0.0938, Val Loss: 0.2835\n",
      "Epoch [291/10000], Train Loss: 0.0935, Val Loss: 0.2830\n",
      "Epoch [292/10000], Train Loss: 0.0932, Val Loss: 0.2825\n",
      "Epoch [293/10000], Train Loss: 0.0929, Val Loss: 0.2821\n",
      "Epoch [294/10000], Train Loss: 0.0925, Val Loss: 0.2816\n",
      "Epoch [295/10000], Train Loss: 0.0923, Val Loss: 0.2812\n",
      "Epoch [296/10000], Train Loss: 0.0920, Val Loss: 0.2808\n",
      "Epoch [297/10000], Train Loss: 0.0917, Val Loss: 0.2805\n",
      "Epoch [298/10000], Train Loss: 0.0914, Val Loss: 0.2800\n",
      "Epoch [299/10000], Train Loss: 0.0911, Val Loss: 0.2795\n",
      "Epoch [300/10000], Train Loss: 0.0908, Val Loss: 0.2791\n",
      "Epoch [301/10000], Train Loss: 0.0905, Val Loss: 0.2787\n",
      "Epoch [302/10000], Train Loss: 0.0902, Val Loss: 0.2784\n",
      "Epoch [303/10000], Train Loss: 0.0899, Val Loss: 0.2779\n",
      "Epoch [304/10000], Train Loss: 0.0896, Val Loss: 0.2775\n",
      "Epoch [305/10000], Train Loss: 0.0893, Val Loss: 0.2771\n",
      "Epoch [306/10000], Train Loss: 0.0891, Val Loss: 0.2767\n",
      "Epoch [307/10000], Train Loss: 0.0888, Val Loss: 0.2763\n",
      "Epoch [308/10000], Train Loss: 0.0885, Val Loss: 0.2759\n",
      "Epoch [309/10000], Train Loss: 0.0882, Val Loss: 0.2755\n",
      "Epoch [310/10000], Train Loss: 0.0879, Val Loss: 0.2750\n",
      "Epoch [311/10000], Train Loss: 0.0877, Val Loss: 0.2747\n",
      "Epoch [312/10000], Train Loss: 0.0874, Val Loss: 0.2743\n",
      "Epoch [313/10000], Train Loss: 0.0871, Val Loss: 0.2738\n",
      "Epoch [314/10000], Train Loss: 0.0868, Val Loss: 0.2735\n",
      "Epoch [315/10000], Train Loss: 0.0866, Val Loss: 0.2731\n",
      "Epoch [316/10000], Train Loss: 0.0863, Val Loss: 0.2727\n",
      "Epoch [317/10000], Train Loss: 0.0860, Val Loss: 0.2723\n",
      "Epoch [318/10000], Train Loss: 0.0858, Val Loss: 0.2719\n",
      "Epoch [319/10000], Train Loss: 0.0855, Val Loss: 0.2717\n",
      "Epoch [320/10000], Train Loss: 0.0853, Val Loss: 0.2711\n",
      "Epoch [321/10000], Train Loss: 0.0850, Val Loss: 0.2708\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://gr032.hpc.nyu.edu:1052/'. Verify the server is running and reachable."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2075/5000], Train Loss: 0.0170, Val Loss: 0.1346\n",
      "Epoch [2076/5000], Train Loss: 0.0169, Val Loss: 0.1346\n",
      "Epoch [2077/5000], Train Loss: 0.0169, Val Loss: 0.1346\n",
      "Epoch [2078/5000], Train Loss: 0.0169, Val Loss: 0.1345\n",
      "Epoch [2079/5000], Train Loss: 0.0169, Val Loss: 0.1345\n",
      "Epoch [2080/5000], Train Loss: 0.0169, Val Loss: 0.1345\n",
      "Epoch [2081/5000], Train Loss: 0.0169, Val Loss: 0.1344\n",
      "Epoch [2082/5000], Train Loss: 0.0169, Val Loss: 0.1344\n",
      "Epoch [2083/5000], Train Loss: 0.0169, Val Loss: 0.1344\n",
      "Epoch [2084/5000], Train Loss: 0.0168, Val Loss: 0.1343\n",
      "Epoch [2085/5000], Train Loss: 0.0168, Val Loss: 0.1343\n",
      "Epoch [2086/5000], Train Loss: 0.0168, Val Loss: 0.1343\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data):\n",
    "    # Prepare test data\n",
    "    test_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_tensor)\n",
    "    return predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_top_k_accuracy(y_pred, y_true, k=5, num_participants=1):\n",
    "    \"\"\"Evaluate top-k accuracy using cosine similarity for embeddings.\"\"\"\n",
    "    num_indices = y_true.shape[0]/num_participants\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(num_participants):\n",
    "        y_pred_participant = y_pred[int(i*num_indices):int((i+1)*num_indices)]\n",
    "        y_true_participant = y_true[int(i*num_indices):int((i+1)*num_indices)]\n",
    "        n_samples = y_true_participant.shape[0]\n",
    "\n",
    "        ## returns a n_samples X n_samples matrix\n",
    "        similarity_matrix = cosine_similarity(y_pred_participant, y_true_participant)\n",
    "        \n",
    "        # Get the indices of the sorted similarities in descending order\n",
    "        sorted_indices = np.argsort(-similarity_matrix, axis=1) #axis = 1 sorts across i.e. sorts across y_true for each pred\n",
    "        \n",
    "        # Check if the true index is within the top-k predictions for each sample\n",
    "        for i in range(n_samples):\n",
    "            if i in sorted_indices[i, :k]:\n",
    "                correct_predictions += 1\n",
    "    \n",
    "    top_k_acc = correct_predictions / y_true.shape[0]\n",
    "    return top_k_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 accuracy: 0.0082\n"
     ]
    }
   ],
   "source": [
    "top_5_acc = evaluate_top_k_accuracy(make_predictions(meg_data_test), embeddings_data_test, k=5, num_participants=4)\n",
    "print(f\"Top-5 accuracy: {top_5_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1325\n"
     ]
    }
   ],
   "source": [
    "top_5_acc = evaluate_top_k_accuracy(make_predictions(meg_data_test_small), embeddings_data_test_small, k=5, num_participants=4)\n",
    "print(top_5_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6777644230769231\n"
     ]
    }
   ],
   "source": [
    "top_5_acc = evaluate_top_k_accuracy(make_predictions(meg_data_train), embeddings_data_train, k=5, num_participants=4)\n",
    "print(top_5_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the PyTorch model\n",
    "torch.save(model.state_dict(), os.path.join(dir_name, 'meg_mlp_fmri_subset.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
