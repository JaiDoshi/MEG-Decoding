{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from simpleconv_diffusion import SimpleConv\n",
    "import torch.optim as optim\n",
    "from classes import MEGDataset\n",
    "from utils import soft_clip_loss, hard_clip_loss, calculate_params, train_modified, val_modified, test_model\n",
    "from mindeye import VersatileDiffusionPriorNetwork, BrainDiffusionPrior\n",
    "from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "from diffusers.models import DualTransformer2DModel\n",
    "import time\n",
    "\n",
    "from diffusion_utils import train_diffusion, val_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    seed = 42\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "    lr = 3e-4\n",
    "    early_stopping = 15\n",
    "    lr_schedule = \"linear\"\n",
    "    warmup_lr = 1e-5\n",
    "    warmup_interval = 1000\n",
    "    loss_func = \"soft_clip_loss\"\n",
    "    output_dir = \"./output\"\n",
    "    wandb_project = \"MEG_Diffusion_Testing\"\n",
    "    wandb_run_name = None\n",
    "    save_interval = 10\n",
    "    print_interval = 50\n",
    "    dilation_type = \"expo\"\n",
    "    dropout = 0.2\n",
    "    embeddings_type = \"vit\"\n",
    "    dataset_type = \"small\"\n",
    "    preprocessing_type = \"raj\"\n",
    "    scheduler_type = \"constant\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using DEVICE:\", DEVICE)\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = (\n",
    "    args.wandb_run_name\n",
    "    if args.wandb_run_name\n",
    "    else f\"Dfsn_Pre{args.preprocessing_type}_Drpt{args.dropout}_Diln{args.dilation_type}_ClipEmb{args.embeddings_type}_Loss[{args.loss_func}]_B{args.batch_size}_LR{args.lr}_S{args.seed}_E{args.epochs}_{now}\"\n",
    ")\n",
    "wandb_run_name = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_type == \"small\" and args.preprocessing_type == \"raj\":\n",
    "    with open('./valid_epochs/valid_epochs_adjusted_train_redid.pickle', 'rb') as f:\n",
    "        valid_epochs = pickle.load(f)\n",
    "    with open('./valid_epochs/valid_epochs_small_test_redid.pickle', 'rb') as f:\n",
    "        valid_epochs_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.embeddings_type == \"vit\":\n",
    "    if args.dataset_type == \"small\":\n",
    "        embeddings = np.load('./embeddings/image_embeddings_vit_hidden.npy', allow_pickle=True).item()\n",
    "        # embeddings_test = np.load('./embeddings/image_embeddings_vit_small_test_redid.npy', allow_pickle=True).item()\n",
    "        embeddings_val = np.array([embeddings[filename] for filename in valid_epochs.metadata['image_path']])\n",
    "        # embeddings_val_test = np.array([embeddings_test[filename] for filename in valid_epochs_test.metadata['image_path']])\n",
    "        embeddings_val_test = np.array([embeddings[filename] for filename in valid_epochs_test.metadata['image_path']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames_set1 = set(valid_epochs.metadata['image_path'])\n",
    "# filenames_set2 = set(embeddings.keys())\n",
    "# if filenames_set1 == filenames_set2:\n",
    "#     print(\"All filenames match!\")\n",
    "# else:\n",
    "#     print(\"Filenames do not exactly match.\")\n",
    "\n",
    "layout = mne.channels.find_layout(valid_epochs.info, ch_type=\"meg\")\n",
    "layout_test = mne.channels.find_layout(valid_epochs_test.info, ch_type='meg')\n",
    "\n",
    "print(\"Train Valid Epochs Shape:\",valid_epochs.get_data().shape)\n",
    "print(\"Test Valid Epochs Shape:\",valid_epochs_test.get_data().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Working on Data Loaders...\")\n",
    "dataset = MEGDataset(valid_epochs, embeddings_val, layout)\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=1)\n",
    "\n",
    "# Test Dataset\n",
    "test_dataset = MEGDataset(valid_epochs_test, embeddings_val_test, layout_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=800, shuffle=False, drop_last=True, num_workers=1)\n",
    "\n",
    "\n",
    "\n",
    "small_train_dataset = torch.utils.data.Subset(\n",
    "    train_dataset, torch.linspace(0, len(train_dataset) - 1, steps=1024).long()) # only 512 data points\n",
    "\n",
    "# 4 batches\n",
    "small_train_loader = DataLoader(\n",
    "    small_train_dataset, batch_size=128, pin_memory=True, shuffle=True)\n",
    "\n",
    "small_val_dataset = torch.utils.data.Subset(\n",
    "    val_dataset, torch.linspace(0, len(val_dataset) - 1, steps=512).long())\n",
    "\n",
    "small_val_loader = DataLoader(\n",
    "    small_val_dataset, batch_size=128, pin_memory=True, shuffle=False)\n",
    "print(\"Data Loaders Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_size = 768 # \"ViT-L/14\": 768\n",
    "norm_embs = True\n",
    "hidden = True\n",
    "prior = True\n",
    "vd_cache_dir = \"./vd_cache/\"  #Where is cached Versatile Diffusion model; if not cached will download to this path\n",
    "n_samples_save = 1\n",
    "lr_scheduler_type = 'linear'\n",
    "\n",
    "args.clip_size = clip_size\n",
    "args.norm_embs = norm_embs\n",
    "args.hidden = hidden\n",
    "args.prior = prior\n",
    "args.vd_cache_dir = vd_cache_dir\n",
    "args.n_samples_save = n_samples_save\n",
    "args.lr_scheduler_type = lr_scheduler_type\n",
    "args.v2c = True\n",
    "\n",
    "if args.hidden:\n",
    "    args.prior_mult = 30.0\n",
    "    args.nce_mult = 0\n",
    "else:\n",
    "    args.prior_mult = .03  # WHY?\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.hidden:\n",
    "        print(\"Using hidden layer CLIP space (Versatile Diffusion)\")\n",
    "        if not args.norm_embs:\n",
    "            print(\"WARNING: YOU WANT NORMED EMBEDDINGS FOR VERSATILE DIFFUSION!\")\n",
    "        # clip_extractor = Clipper(clip_variant, device=device, hidden_state=True, norm_embs=norm_embs)\n",
    "        args.out_dim = 257 * args.clip_size\n",
    "        # OVERWRITE FOR NOW: REMOVE LATER\n",
    "        # args.out_dim = args.clip_size\n",
    "        \n",
    "        print(\"Output Dimension:\", args.out_dim)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instantiating Voxel2Clip Model.. BRAIN NETWORK\")\n",
    "voxel2clip = SimpleConv(\n",
    "    in_channels=272,\n",
    "    out_channels=2048,\n",
    "    merger_dropout=0.2,\n",
    "    hidden_channels=320,\n",
    "    n_subjects=4,\n",
    "    merger=True,\n",
    "    merger_pos_dim=2048,\n",
    "    subject_layers=True,\n",
    "    subject_layers_dim=\"input\",\n",
    "    gelu=True,\n",
    "    device=DEVICE,\n",
    "    dilation_type=args.dilation_type,\n",
    "    extra_dropout=args.dropout,\n",
    "    use_mse_projector=True, ## ADD THIS EXTRA MLP PROJECTOR FOR CLIP AND DIFUSSION PRIOR TO LEARN SEPARATELY\n",
    "    clip_size=args.clip_size,\n",
    "    # projector_dim=2048,\n",
    "    # out_dim=args.out_dim,\n",
    ").to(DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2c_params, v2_trainable_params = calculate_params(voxel2clip)\n",
    "print(\"Voxel2Clip Model Instantiated! Total Parameters:\", v2c_params)\n",
    "print(\"Trainable Parameters:\", v2_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prior network for diffusion prior\n",
    "out_dim = clip_size\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = clip_size//64 # heads * dim_head = 12 * 64 = 768\n",
    "\n",
    "if hidden:\n",
    "    args.guidance_scale = 3.5 #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "    args.timesteps = 100 #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = 257,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        ).to(DEVICE)\n",
    "    print(\"prior_network loaded\")\n",
    "\n",
    "    # custom version that can fix seeds\n",
    "    diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=args.timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "        voxel2clip=voxel2clip, # THIS IS OUR BRAIN MODULE , TRAINED END TO END WITH DIFFUSION PRIOR\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "\n",
    "if not prior:\n",
    "    diffusion_prior = diffusion_prior.requires_grad_(False)\n",
    "    diffusion_prior.voxel2clip.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Diffusion Prior  Instantiated! Total Parameters:\", calculate_params(diffusion_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_samples_save > 0 and args.hidden:\n",
    "        print('Creating versatile diffusion reconstruction pipeline...')\n",
    "        try:\n",
    "            vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(vd_cache_dir).to('cpu')\n",
    "        except:\n",
    "            print(\"Downloading Versatile Diffusion to\", vd_cache_dir)\n",
    "            vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "                    \"shi-labs/versatile-diffusion\",\n",
    "                    cache_dir = vd_cache_dir).to('cpu')\n",
    "        vd_pipe.image_unet.eval()\n",
    "        vd_pipe.vae.eval()\n",
    "        vd_pipe.image_unet.requires_grad_(False)\n",
    "        vd_pipe.vae.requires_grad_(False)\n",
    "        path_scheduler = \"./vd_cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7/scheduler/scheduler_config.json\"\n",
    "        vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(path_scheduler)\n",
    "        args.num_inference_steps = 20 #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "\n",
    "        # Set weighting of Dual-Guidance \n",
    "        text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "        for name, module in vd_pipe.image_unet.named_modules():\n",
    "            if isinstance(module, DualTransformer2DModel):\n",
    "                module.mix_ratio = text_image_ratio\n",
    "                for i, type in enumerate((\"text\", \"image\")):\n",
    "                    if type == \"text\":\n",
    "                        module.condition_lengths[i] = 77\n",
    "                        module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "                    else:\n",
    "                        module.condition_lengths[i] = 257\n",
    "                        module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "                        \n",
    "        # args.unet = vd_pipe.image_unet #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "        # args.vae = vd_pipe.vae #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "        # args.noise_scheduler = vd_pipe.scheduler #NEED TO BE PASSED FOR RECONSTRUCTION\n",
    "        # args.vd_pipe = vd_pipe #NEED TO BE PASSED FOR RECONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Instantiated!, Model device:\", diffusion_prior.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_loss, val_loss, best_model_path = train_diffusion(\n",
    "        diffusion_prior, train_loader, val_loader, args, DEVICE, vd_pipe\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
